\section{Evaluation}

%The problem with evaluating preprocessing pipelines is, that we lack ground truth.
%Therefore any approach learning is potentially prone to overfitting.
We evaluate the quality of the registration both in terms of spatial features, as well as in terms of its repercussion on higher-level functional analysis.
%with our novel workflow or with the Here we derive two novel metrics for assessing the quality of the registration, which were not used by engineering the new workflow nor used in the optimisation procedure. 

A main challenge of QC with regard to spatial features is that a perfect remapping is undefined.
Similarity metrics are ill-fitted for QC because they are used internally by registration functions, whose main feature it is, that they maximize them.
Indeed an extreme maximization, especially via nonlinear transformations, results in a distortion of the image, which should be penalized in QC, but in light of image similarity scores, is represented as better performance.
Additionally, similarity metrics are not independent, so this issue cannot be circumvented by maximizing a subset of metrics and performing QC in light of the remainder.
We thus develop three alternative evaluation metrics: volume conservation, functional analysis, and variance analysis.

\subsection{Volume Conservation}

We have developed a simple, fast, and widely applicable metric to measure distortion introduced by preprocessing workflows.
Volume conservation is based on the assumption that the total volume of the scanned segment of the brain should remain approximately identical after registration.
A volume increase may indicate that the brain was stretched to fill in template brain space not covered by the scan, while a volume decrease might indicate that non-brain voxels were introduced into the template brain space.

Reference brain volume is estimated as the 66\textsuperscript{th} percentile of the unregistered scan.
The arbitrary unit equivalent of this percentile threshold is recorded for each scan and applied to all registration workflow results for that scan, to obtain transformed brain volume estimates.
In order to mitigate possible differences arising from template size, we perform a multivariate analysis of both template and analysis.
In order to best analyze volume conservation, a Volume Change Factor (VCF) is computed for each processed scan, whereby volume conservation is highest for a VCF equal to 1.

% This metric provides distortion checking rather than goodness-of-fit qantification, which is, as previously described, difficult to do in a lean automated fashion.

\begin{sansmath}
\py{pytex_subfigs(
        [
                {'script':'scripts/vc_violin.py', 'label':'vcv', 'conf':'article/1col.conf', 'options_pre':'{.48\\textwidth}',
			'caption':'Comparison across workflows and target templates, considering both BOLD and CBV both functional contrasts.'
                        ,},
                {'script':'scripts/vcc_violin.py', 'label':'vccv','conf':'article/1col.conf', 'options_pre':'{.48\\textwidth}',
                        'caption':'Comparison across workflows and functional contrasts, considering only matching template-workflow combinations.'
                        ,},
                ],
        caption='Volume change relative to the original scan volume. Coloured patch width estimates distribution density, while continuous markers indicate the sample mean and dashed markers indicate the inner quartiles.',
        label='fig:vc',)}
\end{sansmath}

As seen in \cref{fig:vcv}, we note that the Volume Change Factor (VCF) metric is sensitive to both
the processing workflow (\py{boilerplate.fstatistic('Processing', condensed=True)}),
the template (\py{boilerplate.fstatistic('Template', condensed=True)}),
and interactions thereof (\py{boilerplate.fstatistic('Processing:Template', condensed=True)}).

Testing the core hypothesis of the comparison ---
whether the Generic SAMRI workflow (with the Generic template) performs significantly different than the Legacy workflow (with the Legacy template) ---
we note that it does
(two-tailed p-value of \py{pytex_printonly('scripts/vc_t.py')}).
Additionally we note a root mean squared error ratio strongly favouring the Generic workflow
($\mathrm{RMSE_{L}/RMSE_{G}\simeq} \py{pytex_printonly('scripts/vc_rmser.py')}$).

Descriptively, we observe that the effect with the greatest magnitude is that of the template variable, with its Legacy level introducing a notable volume loss
(VCF of \py{boilerplate.vc_factorci('Template[T.Legacy]')}).
Further, we note that there is a variance increase in all conditions for the Legacy processing workflow
(\py{boilerplate.varianceratio(template='Legacy')}-fold given the Legacy template, and \py{boilerplate.varianceratio(template='Generic')}-fold given the Generic template).

With respect to the data break-up by contrast (from \cref{fig:vccv}), we see no notable main effect for the contrast variable
(VCF of \py{boilerplate.vcc_factorci('Contrast[T.CBV]')}).
We do, however, report a notable effect for the contrast-template interaction, with the Legacy workflow and CBV contrast interaction level introducing a volume loss
(VCF of \py{boilerplate.vcc_factorci('Processing[T.Legacy]:Contrast[T.CBV]')}).

\subsection{Functional Analysis}

Functional analysis, successfully circumvents the issue, as the metric being maximized in the registration process is not the same metric used for QC.
This method is however primarily suited to demonstrate workflow relevance to higher-level applications.

\subsection{Variance}

\[ CC(x) = \frac{\sum_i{(x)}}{\sum_i{(x)}} \]

%\py{pytex_fig('scripts/registration_qc.py', conf='article/varplot.conf', label='varplot', caption='Variance for different preprocessing pipelines')}

To assess the quality of the pipeline we evaluated the registration performance for different metrices (Crosscorrelation (CC), Mutual Information (MI), Mean Squared Difference (MSQ)) for individual sessions and subjects on a representatitve dataset.  We define an assessment for registration quality based on the assumption, that for increased registration quality the variance of a similariy measure between the subject and the template should converge towards 0. This definition is based on the assumption that biological deformations of the brain across sessions should be negligible (Ref???!?!). Hence we calculate the variance over different similiarity metrics for each subject across sessions. We average for each workflow the results across subjects. We find that our new preprocessing pipeline has significantly less variance than the legacy workflow, while the optimised pipeline has even further decreased variance.
