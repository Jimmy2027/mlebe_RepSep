\section{Evaluation}
%%% I think classification is more sensible than segmentation (you really only segment data into two), but I might be wrong if this is some specific ML jargon where there is some meaningful difference between the terms. If not and if a specific term has synonyms, try to just choose one, and stick with it.
For the quality control of the workflow, we first evaluate the classification process, followed by a benchmark between the Generic and the improved (Generic Masked) workflow.


\subsection{Segmentation}
%%% Again, background or discussion
Quality control of our classifier is difficult in the sense that it should predict a better mask than the template.
Nevertheless, it is useful to verify whether the output is similar, as it should be.
%%% A similarity metric for what, between what and what?
As a similarity metric we have used the Dice score (see \cref{eqDcoef}).
The average Dice score taken on every slice of the test data set is $D_{coef}= $ \py{boilerplate.print_dice()} $\sim 1$.
%%% Sentences evaluating your evaluation results go into Discussion, if you *really* want to make a point here, add it as a sub-clause, e.g. “The dice score average shows strong similarity between y and x, yielding a value of XXXXX”
The predictions thus have only minor changes in comparison with the template.


\begin{sansmath}
\py{pytex_fig('scripts/classifier/plt_testset_examples.py',
        conf='article/wide.conf',
        label='testset_ex',
        caption='
                \\textbf{The Classifier predicts a similar mask to the ground truth.}
                Randomly picked plots from the test set illustrate the predictions of the classifier.
                The first row presents the input image, the second the ground truth and the third row shows the predictions of the classifier.
                ',
        multicol=True,
        )}
\end{sansmath}

%%% Kind of repetitive since you already talked about this at the start of the section...
We evaluate the effects of our classifier on a full-fledged registration workflow via the benchmarking algorithms from \cite{ioanas_optimized_2019}.
%%% Generic Masked is really more descriptive, but you should introduce the terminology when you introduce the workflow.
We denote the original workflow as Generic and our modified version as Generic Masked.

\subsection{Workflow}

%%% Don't spell out the author name, just include the citation, and if you want it to expand to list the name (which I do not like, but your and the journal's preference have priority over mine :D), you can do that via bibtex.
%%% Also this is background stuff
%As stated by Ioanas et al. in \cite{ioanas_optimized_2019} a major challenge of registration quality control (QC) is that a perfect mapping from the measured image to the template is undefined.
%To address this challenge, they developed four alternative evaluation metrics: volume conservation, smoothness conservation, functional analysis, and variance analysis.
We use an established palette of workflow evaluation metrics --- inspecting volume and smoothness conservation, as well as downstream effects on basic functional analysis \cite{ioanas_optimized_2019} --- to benchmark the novel SAMRI Generic Masked workflow against the SAMRI Generic workflow.
%%% Awwww, thanks :"> , but this is (1) unscientifically formulated, (2) background or discussion content
It is worth noting that the quality of the Generic workflow is already remarkable.
Even slight improvements should be considered an amelioration.
%%% Introduce RMSE bootstrapping here, as you will need it for both VCF and SCF.

\subsection{Volume Conservation}

\begin{sansmath}
\py{pytex_subfigs(
        [
                {'script':'scripts/vcc_violin.py', 'label':'vccv','conf':'article/1col.conf', 'options_pre':'{.48\\textwidth}',
                        'options_pre_caption':'\\vspace{-1.5em}\\',
                        'options_post':'\\vspace{1em}',
                        'caption':'Comparison across workflows and functional contrasts.'
                        ,},
                {'script':'scripts/scf_violin_contrasts.py', 'label':'sccv','conf':'article/1col.conf', 'options_pre':'{.48\\textwidth}',
                        'options_pre_caption':'\\vspace{-1.5em}\\',
                        'options_post':'\\vspace{1em}',
                        'caption':'Comparison across workflows and functional contrasts.'
                        ,},
                ],
        caption='\\textbf{Both the SAMRI Generic and the Generic* workflow optimally and reliably conserve volume and smoothness, the latter showing values that are closely distributed to 1.}
        Plots of three target metrics, with coloured patch widths estimating distribution density, solid lines indicating the sample mean, and dashed lines indicate the inner quartiles.
        ',
        label='fig:vc',
        )}
\end{sansmath}

%%% Ughhhh... please don't copy text from IRSABI (not for my sake, but four yours, technically someone who really wants to find fault with your work could construe this as plagiarism). Briefly rephrase what this is, maybe show the equation, and reference IRSABI --- and you're done!
Volume conservation is based on the assumption that the total volume of the scanned segment of the brain should remain roughly constant after preprocessing.
Beyond just size differences between the acquired data and the target template, a volume increase may indicate that the brain was stretched to fill in template brain space not covered by the scan, while a volume decrease might indicate that non-brain voxels were introduced into the template brain space.
For this analysis, we compute a Volume Conservation Factor (VCF), whereby volume conservation is highest for a VCF equal to 1.

%%% Ok, you introduce the VCF and you show the VCF plots primarily, and then you list the stats for RMSE only? You should definitely write out the modelling evaluation for VCF. My recommendation is to write down a paragraph for the VCF evaluation, and then RMSE evaluation, plot-wise you show a composite figure with subfigures for VCF, SCF, and the respective bootstrapped RMSE.

%%% briefly introduce what the RMSE tells you here.
Evaluating a bootstrapped distribution of the respective Root Mean Squared Errors (RMSE) to 1, we report that the RMSE is sensitive to the
workflow (RMSE of \py{boilerplate.corecomparison_factorci('Processing[T.Generic Masked]', df_path='data/bootstrapped_volume.csv', dependent_variable='RMSE')})
and the contrast-workflow interaction (RMSE of \py{boilerplate.corecomparison_factorci('Processing[T.Generic Masked]:Contrast[T.CBV]', df_path='data/bootstrapped_volume.csv', dependent_variable='RMSE')}).

A Wilcoxon signed-rank test on the absolute distance to 1 of both VCF distributions shows that the VCF is sensitive to the workflow (rank = \py{boilerplate.wilcoxon_(dependent_variable='1 - Vcf')[0]}, p = \py{boilerplate.wilcoxon_(dependent_variable='1 - Vcf')[1]}).
%As seen in \cref{fig:vccv}, we note that in the described dataset VCF is sensitive to the workflow (\py{boilerplate.fstatistic('Processing', condensed=True)}),
%but not the interaction of the workflow with the contrast (\py{boilerplate.fstatistic('Processing:Contrast', condensed=True)}).
%Against : (\py{boilerplate.fstatistic('Processing', condensed=True, dependent_variable  = '1 - Vcf')}, \py{boilerplate.fstatistic('Processing:Contrast', condensed=True, dependent_variable  = '1 - Vcf')})

%
%The performance of the Generic SAMRI workflow is different from that of Generic*, yielding a two-tailed p-value of \py{pytex_printonly('scripts/vc_t.py')}.
Moreover, the root mean squared error ratio favours the Generic* workflow
($\mathrm{RMSE_{G*}/RMSE_{G}\simeq} \py{pytex_printonly('scripts/vc_rmser.py')}$).

%Descriptively, we observe that the Generic* level of the workflow variable introduces a volume gain
%(VCF of \py{boilerplate.factorci('Processing[T.Generic Masked]')}).
Further, we note that there is a significant variance decrease in all conditions for the Generic Masked workflow
(\py{boilerplate.varianceratio()}-fold).

With respect to the data break-up by contrast (CBV versus BOLD, \cref{fig:vccv}), we see no notable main effect for the contrast variable
(VCF of \py{boilerplate.corecomparison_factorci('Contrast[T.CBV]')}), nor do we report a notable effect for the contrast-workflow interaction (VCF of \py{boilerplate.corecomparison_factorci('Processing[T.Generic Masked]:Contrast[T.CBV]')}).

\subsection{Smoothness Conservation}

%%% Same comments as for VCF

A further aspect of preprocessing quality is the resulting image smoothness.
Although controlled smoothing is a valuable preprocessing tool used to increase the signal-to-noise ratio (SNR), uncontrolled smoothness limits operator discretion in the trade-off between SNR and feature granularity.
Uncontrolled smoothness can thus lead to undocumented and implicit loss of spatial resolution and is therefore associated with inferior anatomical alignment \cite{fmriprep}.
We employ a Smoothness Conservation Factor (SCF), expressing the ratio between the smoothness of the preprocessed images and the smoothness of the original images.

While the performance of the Generic SAMRI workflow is only slightly different from that of the Generic* workflow, the root mean squared error ratio favors the Generic* workflow ($\mathrm{RMSE_{G*}/RMSE_{G}\simeq} \py{pytex_printonly('scripts/scf_rmser.py')}$).

Descriptively, we observe that neither the Generic nor the Generic* workflow introduce a strong smoothing (SCF of \py{boilerplate.factorci('Processing[T.Generic Masked]', df_path='data/smoothness.csv', dependent_variable='Smoothness Conservation Factor')}).

Further, we note that there is a notable variance decrease for the Generic* workflow
(\py{boilerplate.varianceratio(df_path='data/smoothness.csv',dependent_variable='Smoothness Conservation Factor', max_len=3)}
-fold).

Given the break-up by contrast shown in \cref{fig:sccv}, we see no effect for the contrast variable
(SCF of \py{boilerplate.corecomparison_factorci('Contrast[T.CBV]', df_path='data/smoothness.csv', dependent_variable='Smoothness Conservation Factor')})
and the contrast-workflow interaction
(SCF of \py{boilerplate.corecomparison_factorci('Processing[T.Generic Masked]:Contrast[T.CBV]', df_path='data/smoothness.csv', dependent_variable='Smoothness Conservation Factor')}).

\subsection{Functional Analysis}

%%% Same comments as for VCF, these three sections can basically be copy-pasted in as far as the sentence structure is concerned.

Functional analysis is a frequently used avenue for preprocessing QC.
Its viability derives from the fact that the metric being maximized in the registration process is not the same output metric as that used for QC.
The method is however primarily suited to examine workflow effects in light of higher-level applications, and less suited for wide-spread QC (as it is computationally intensive and only applicable to stimulus-evoked functional data).
%Additionally, functional analysis significance is documented to be sensitive to data smoothness \cite{Molloy2014}, and thus an increased score on account of uncontrolled smoothing can be expected.
For this analysis we compute the Mean Significance (MS), expressing the significance detected across all voxels of a scan.
%The performance of the SAMRI Generic workflow differs significantly from that of the Generic* workflow in terms of MS, yielding a two-tailed p-value of \py{pytex_printonly('scripts/ms_t.py')}.

We observe that the Generic* level of the workflow variable does not introduce a notable significance loss
(MS of \py{boilerplate.factorci('Processing[T.Generic Masked]', df_path='data/functional_significance.csv', dependent_variable='Mean Significance')}).
Furthermore, we note a slight variance decrease in all conditions for the Generic* workflow
(\py{boilerplate.varianceratio(df_path='data/functional_significance.csv', dependent_variable='Mean Significance')}-fold).

With respect to the data break-up by contrast (\cref{fig:mscv}), we see no notable main effect for the contrast variable
(MS of \py{boilerplate.corecomparison_factorci('Contrast[T.CBV]', df_path='data/functional_significance.csv', dependent_variable='Mean Significance')}).
%and no notable effect for the contrast-template interaction
%(MS of \py{boilerplate.corecomparison_factorci('Processing[T.Legacy]:Contrast[T.CBV]', df_path='data/functional_significance.csv', dependent_variable='Mean Significance')}).

%Functional analysis effects can further be inspected by visualizing the statistic maps.
%Second-level t-statistic maps depicting the CBV and BOLD omnibus contrasts (common to all subjects and sessions) provide a succinct overview capturing both amplitude and directionality of the signal (\cref{fig:m}).
%%While the most salient feature of this figure is the qualitative distribution difference between CBV and BOLD scans, we note that this is to be expected given the different nature of the processes, and is wholly orthogonal to the question of registration.
%The differential coverage is crucial to the examination of registration quality and its effects on functional read-outs.
%We note that processing with the Generic* workflow (\cref{fig:mllc,fig:mllb}), does not induce issues with statistic coverage alignment and overflow.

\subsection{Variance Analysis}

\begin{sansmath}
\py{pytex_fig('scripts/variance_catplot.py',
        conf='article/varplot.conf',
        label='var',
        caption='
                \\textbf{The SAMRI Generic* workflow conserves subject-wise variability and minimizes trial-to-trial variability.}
                Swarmplots illustrate similarity metric scores of preprocessed images with respect to the corresponding workflow template, plotted across subjects (separated into x-axis bins) and sessions (individual points in each x-axis bin), for the CBV contrast.
                ',
        multicol=True,
        )}
\end{sansmath}

%%% Again, please don't copy verbatim from IRSABI

An additional way to assess preprocessing quality focuses on the robustness to variability across repeated measurements, and whether this is attained without overfitting (i.e. compromising physiologically meaningful variability).
The core assumption of this analysis of variance is that adult mouse brains in the absence of intervention retain size, shape, and implant position throughout the 8 week study period.
Consequently, when examining similarity scores of preprocessed scans with respect to the target template, more variation should be found across levels of the subject variable rather than the session variable.
This comparison can be performed using a type 3 ANOVA, modeling both the subject and the session variables.
For this assessment we select three metrics with maximal sensitivity to different features:
Neighborhood Cross Correlation (CC, sensitive to localized correlation),
Global Correlation (GC, sensitive to whole-image correlation),
and Mutual Information (MI, sensitive to whole-image information similarity).
%As similarity metrics are not equivalent, and since the main comparison of variance is performed across all of them, p-values for the variable effects are uncorrected.

\Cref{fig:var} renders the similarity metric scores for both the SAMRI Generic and Generic* workflows.
Both, the Generic and the Generic* workflow produce results which show a higher F-statistic for the subject than for the session variable.
For the Generic* workflow, F-statistics show:
CC (subject: \py{boilerplate.variance_test('C(Subject)','Generic Masked','CC', condensed=True)}, session: \py{boilerplate.variance_test('C(Session)','Generic Masked','CC', condensed=True)}),
GC (subject: \py{boilerplate.variance_test('C(Subject)','Generic Masked','GC', condensed=True)}, session: \py{boilerplate.variance_test('C(Session)','Generic Masked','GC', condensed=True)}),
and MI (subject: \py{boilerplate.variance_test('C(Subject)','Generic Masked','MI', condensed=True)}, session: \py{boilerplate.variance_test('C(Session)','Generic Masked','MI', condensed=True)}).

%%% What do you mean by “the same trend”?
For the Generic SAMRI workflow, resulting data F-statistics follow the same trend:
CC (subject: \py{boilerplate.variance_test('C(Subject)','Generic','CC', condensed=True)}, session: \py{boilerplate.variance_test('C(Session)','Generic','CC', condensed=True)}),
GC (subject: \py{boilerplate.variance_test('C(Subject)','Generic','GC', condensed=True)}, session: \py{boilerplate.variance_test('C(Session)','Generic','GC', condensed=True)}),
and MI (subject: \py{boilerplate.variance_test('C(Subject)','Generic','MI', condensed=True)}, session: \py{boilerplate.variance_test('C(Session)','Generic','MI', condensed=True)}).
